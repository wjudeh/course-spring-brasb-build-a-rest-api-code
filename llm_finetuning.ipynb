{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "collapsed_sections": [
        "R4gFkGQA4_BW"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wjudeh/course-spring-brasb-build-a-rest-api-code/blob/main/llm_finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Fine-Tuning Large Language Models: A Practical Guide**\n",
        "\n",
        "#### **Introduction**\n",
        "\n",
        "Welcome to this tutorial on **fine-tuning large language models (LLMs)**! In this hands-on guide, you will learn how to fine-tune a **pre-trained language model** using **LLaMA-Factory**, **Hugging Face Transformers**, and **vLLM**. The tutorial provides step-by-step instructions for data preparation, model fine-tuning, evaluation, and deployment.\n",
        "\n",
        "#### **What You Will Learn**\n",
        "\n",
        "By the end of this tutorial, you will understand:\n",
        "\n",
        "---\n",
        "\n",
        "- **Setting Up the Environment**: Installing necessary libraries and dependencies for fine-tuning.\n",
        "- **Dataset Preparation**: Formatting data for **Supervised Fine-Tuning (SFT)** using Pydantic schemas.\n",
        "- **Fine-Tuning with LLaMA-Factory**: Training a model on a dataset, configuring **LoRA adapters**, and logging experiments with **Weights & Biases (WandB)**.\n",
        "- **Evaluation and Inference**: Using the fine-tuned model for **Arabic news story processing**, including **entity extraction** and **translation**.\n",
        "- **Optimized Inference with vLLM**: Deploying the model with **vLLM for fast inference** and testing its performance.\n",
        "- **Cost Estimation**: Evaluating the token consumption and cost-effectiveness of fine-tuning.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Who Should Take This Tutorial?**\n",
        "\n",
        "This tutorial is designed for:\n",
        "\n",
        "- **Machine Learning Engineers** who want to fine-tune LLMs efficiently.\n",
        "- **NLP Practitioners** working with Arabic text processing.\n",
        "- **AI Enthusiasts** looking to understand **parameter-efficient fine-tuning (LoRA)**.\n",
        "- **Developers** interested in **deploying optimized LLMs using vLLM**.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Prerequisites**\n",
        "\n",
        "Before starting, ensure you have:\n",
        "\n",
        "- Basic knowledge of **Python** and **NLP**.\n",
        "- Familiarity with **Hugging Face Transformers**.\n",
        "- Access to a **GPU-enabled environment** (Google Colab, AWS, etc.).\n",
        "\n",
        "---\n",
        "\n",
        "This tutorial provides a **practical workflow** for fine-tuning LLMs with real-world examples. Get ready to explore **cutting-edge NLP techniques** and optimize your models for better performance! ğŸš€\n",
        "\n",
        "Author:\n",
        "- [Abu Bakr Soliman](https://www.linkedin.com/in/bakrianoo/)"
      ],
      "metadata": {
        "id": "aI6gDNdb51aO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mount Your Google Drive"
      ],
      "metadata": {
        "id": "OicYSU_s6xzM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzExaTCGnZRF"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "zDV3lCZHxwOe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU transformers==4.48.3 datasets==3.2.0 optimum==1.24.0\n",
        "!pip install -qU openai==1.61.0 wandb\n",
        "!pip install -qU json-repair==0.29.1\n",
        "!pip install -qU faker==35.2.0\n",
        "!pip install -qU vllm==0.7.2"
      ],
      "metadata": {
        "id": "vhrM7Kgyxydu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\n",
        "!cd LLaMA-Factory && pip install -e ."
      ],
      "metadata": {
        "id": "8526F9lVKilu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import wandb\n",
        "\n",
        "wandb.login(key=userdata.get('wandb'))\n",
        "hf_token = userdata.get('huggingface')\n",
        "!huggingface-cli login --token {hf_token}"
      ],
      "metadata": {
        "id": "65ZhrnavyL0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "uLdOa2oSyuBE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "from os.path import join\n",
        "import random\n",
        "from tqdm.auto import tqdm\n",
        "import requests\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List, Optional, Literal\n",
        "from datetime import datetime\n",
        "\n",
        "import json_repair\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "data_dir = \"/gdrive/MyDrive/youtube-resources/llm-finetuning\"\n",
        "base_model_id = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "\n",
        "device = \"cuda\"\n",
        "torch_dtype = None\n",
        "\n",
        "def parse_json(text):\n",
        "    try:\n",
        "        return json_repair.loads(text)\n",
        "    except:\n",
        "        return None"
      ],
      "metadata": {
        "id": "y8utsMq7y_TP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tasks"
      ],
      "metadata": {
        "id": "UDKfM9Ua00SH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "story = \"\"\"\n",
        "Ø°ÙƒØ±Øª Ù…Ø¬Ù„Ø© ÙÙˆØ±Ø¨Ø³ Ø£Ù† Ø§Ù„Ø¹Ø§Ø¦Ù„Ø© ØªÙ„Ø¹Ø¨ Ø¯ÙˆØ±Ø§ Ù…Ø­ÙˆØ±ÙŠØ§ ÙÙŠ ØªØ´ÙƒÙŠÙ„ Ø¹Ù„Ø§Ù‚Ø© Ø§Ù„Ø£ÙØ±Ø§Ø¯ Ø¨Ø§Ù„Ù…Ø§Ù„ØŒ\n",
        " Ø­ÙŠØ« ØªØªØ£Ø«Ø± Ù‡Ø°Ù‡ Ø§Ù„Ø¹Ù„Ø§Ù‚Ø© Ø¨Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø³Ù„ÙˆÙƒ Ø§Ù„Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…ØªÙˆØ§Ø±Ø«Ø© Ø¹Ø¨Ø± Ø§Ù„Ø£Ø¬ÙŠØ§Ù„.\n",
        "\n",
        "Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø°ÙŠ ÙŠØ³ØªÙ†Ø¯ Ø¥Ù„Ù‰ Ø£Ø¨Ø­Ø§Ø« Ø§Ù„Ø£Ø³ØªØ§Ø° Ø§Ù„Ø¬Ø§Ù…Ø¹ÙŠ Ø´Ø§ÙŠÙ† Ø¥Ù†ÙŠØª Ø­ÙˆÙ„\n",
        "Ø§Ù„Ø±ÙØ§Ù‡ Ø§Ù„Ù…Ø§Ù„ÙŠ ÙŠÙˆØ¶Ø­ Ø£Ù† Ù„ÙƒÙ„ Ø´Ø®Øµ \"Ø´Ø®ØµÙŠØ© Ù…Ø§Ù„ÙŠØ©\" ØªØªØ­Ø¯Ø¯ ÙˆÙÙ‚Ø§ Ù„Ø·Ø±ÙŠÙ‚Ø©\n",
        " ØªÙØ§Ø¹Ù„Ù‡ Ù…Ø¹ Ø§Ù„Ù…Ø§Ù„ØŒ ÙˆØ§Ù„ØªÙŠ ØªØªØ£Ø«Ø± Ø¨Ø´ÙƒÙ„ Ù…Ø¨Ø§Ø´Ø± Ø¨ØªØ±Ø¨ÙŠØ© Ø§Ù„Ø£Ø³Ø±Ø© ÙˆØªØ¬Ø§Ø±Ø¨ Ø§Ù„Ø·ÙÙˆÙ„Ø©.\n",
        "\n",
        " Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯ Ø§Ù„Ø«Ù„Ø§Ø«Ø© Ù„Ù„Ø¹Ù„Ø§Ù‚Ø© Ø¨Ø§Ù„Ù…Ø§Ù„\n",
        "Ø¨Ø­Ø³Ø¨ Ø§Ù„Ø¯Ø±Ø§Ø³Ø©ØŒ Ù‡Ù†Ø§Ùƒ Ø«Ù„Ø§Ø«Ø© Ø£Ø¨Ø¹Ø§Ø¯ Ø±Ø¦ÙŠØ³ÙŠØ© ØªØ´ÙƒÙ‘Ù„ Ø¹Ù„Ø§Ù‚ØªÙ†Ø§ Ø¨Ø§Ù„Ù…Ø§Ù„:\n",
        "\n",
        "Ø§Ù„Ø§ÙƒØªØ³Ø§Ø¨ (A): ÙŠÙ…ÙŠÙ„ Ø§Ù„Ø£ÙØ±Ø§Ø¯ Ø§Ù„Ø°ÙŠÙ† ÙŠÙ†ØªÙ…ÙˆÙ† Ù„Ù‡Ø°Ø§\n",
        " Ø§Ù„Ø¨Ø¹Ø¯ Ø¥Ù„Ù‰ Ø§Ø¹ØªØ¨Ø§Ø± Ø§Ù„Ù…Ø§Ù„ Ø³Ù„Ø¹Ø© Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„Ø¬Ù…Ø¹ØŒ Ø­ÙŠØ« ÙŠØ±ÙˆÙ†\n",
        "ÙÙŠ ØªØ­Ù‚ÙŠÙ‚ Ø§Ù„Ø«Ø±ÙˆØ© Ù‡Ø¯ÙØ§ Ø¨Ø­Ø¯ Ø°Ø§ØªÙ‡. ÙˆØ§Ù„Ø¬Ø§Ù†Ø¨ Ø§Ù„Ø³Ù„Ø¨ÙŠ Ù„Ù‡Ø°Ø§\n",
        " Ø§Ù„Ù†Ù…Ø· Ù‡Ùˆ Ø¥Ù…ÙƒØ§Ù†ÙŠØ© Ø§Ù„ØªØ­ÙˆÙ„ Ø¥Ù„Ù‰ Ù‡ÙˆØ³ Ø¨Ø§Ù„Ø«Ø±ÙˆØ© Ø£Ùˆ Ø§Ù„Ø¹ÙƒØ³ØŒ\n",
        " Ø£ÙŠ Ø±ÙØ¶ ØªØ§Ù… Ù„Ø§ÙƒØªØ³Ø§Ø¨ Ø§Ù„Ù…Ø§Ù„ Ø¨Ø§Ø¹ØªØ¨Ø§Ø±Ù‡ Ù…ØµØ¯Ø±Ø§ Ù„Ù„ÙØ³Ø§Ø¯.\n",
        "\n",
        "Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… (U): ÙŠØ±Ù‰ Ù‡Ø¤Ù„Ø§Ø¡ Ø§Ù„Ø£Ø´Ø®Ø§Øµ Ø§Ù„Ù…Ø§Ù„ Ø£Ø¯Ø§Ø© Ù„Ù„ØªÙ…ØªØ¹ Ø¨Ø§Ù„Ø­ÙŠØ§Ø©ØŒ Ø­ÙŠØ« ÙŠØ±Ø¨Ø·ÙˆÙ† Ù‚ÙŠÙ…ØªÙ‡ Ø¨Ù‚Ø¯Ø±ØªÙ‡ Ø¹Ù„Ù‰ ØªÙˆÙÙŠØ±\n",
        "Ø§Ù„Ù…ØªØ¹Ø© ÙˆØ§Ù„Ø±Ø§Ø­Ø©. ÙˆÙ…Ø¹ Ø°Ù„ÙƒØŒ Ù‚Ø¯ ÙŠØµØ¨Ø­\n",
        "Ø§Ù„Ø¨Ø¹Ø¶ Ù…Ø¯Ù…Ù†Ø§ Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ù†ÙØ§Ù‚ØŒ ÙÙŠ Ø­ÙŠÙ† ÙŠØªØ¬Ù‡ Ø¢Ø®Ø±ÙˆÙ† Ø¥Ù„Ù‰ Ø§Ù„ØªÙ‚Ø´Ù Ø§Ù„Ù…ÙØ±Ø· Ø®ÙˆÙØ§ Ù…Ù† Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„.\n",
        "\n",
        "Ø§Ù„Ø¥Ø¯Ø§Ø±Ø© (M): Ø£ØµØ­Ø§Ø¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…Ø· ÙŠØ¹ØªØ¨Ø±ÙˆÙ† Ø§Ù„Ù…Ø§Ù„ Ù…Ø³Ø¤ÙˆÙ„ÙŠØ© ØªØªØ·Ù„Ø¨ Ø§Ù„ØªØ®Ø·ÙŠØ· Ø§Ù„Ø¯Ù‚ÙŠÙ‚. Ù„ÙƒÙ† ÙÙŠ Ø¨Ø¹Ø¶ Ø§Ù„Ø­Ø§Ù„Ø§ØªØŒ\n",
        " Ù‚Ø¯ ÙŠØªØ­ÙˆÙ„ Ø§Ù„Ø£Ù…Ø± Ø¥Ù„Ù‰ Ù‡ÙˆØ³ Ù…ÙØ±Ø· Ø¨Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø¥Ù†ÙØ§Ù‚ØŒ Ù…Ù…Ø§ ÙŠØ¤Ø«Ø± Ø³Ù„Ø¨Ø§ Ø¹Ù„Ù‰ Ø§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª Ø§Ù„Ø´Ø®ØµÙŠØ©.\n",
        "\n",
        " ÙƒÙŠÙ ØªØ¤Ø«Ø± Ø§Ù„Ø¹Ø§Ø¦Ù„Ø© Ø¹Ù„Ù‰ Ø¹Ù„Ø§Ù‚ØªÙ†Ø§ Ø¨Ø§Ù„Ù…Ø§Ù„ØŸ\n",
        "ÙŠØ´ÙŠØ± Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø¥Ù„Ù‰ Ø£Ù† Ø§Ù„ØªØ¬Ø§Ø±Ø¨ Ø§Ù„Ø£Ø³Ø±ÙŠØ© ØªÙ„Ø¹Ø¨ Ø¯ÙˆØ±Ø§ Ø±Ø¦ÙŠØ³ÙŠØ§ ÙÙŠ ØªØ­Ø¯ÙŠØ¯\n",
        " \"Ø§Ù„Ø´Ø®ØµÙŠØ© Ø§Ù„Ù…Ø§Ù„ÙŠØ©\" Ù„ÙƒÙ„ ÙØ±Ø¯ØŒ Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø£Ø­Ø¯ Ø§Ù„ÙˆØ§Ù„Ø¯ÙŠÙ† ÙŠØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø§Ù„\n",
        "ÙƒÙ…ÙƒØ§ÙØ£Ø© Ù„Ù„Ø³Ù„ÙˆÙƒ Ø§Ù„Ø¬ÙŠØ¯ØŒ ÙÙ‚Ø¯ ÙŠØªØ¨Ù†Ù‰ Ø§Ù„Ø·ÙÙ„ Ù„Ø§Ø­Ù‚Ø§ Ø§Ù„Ù†Ù…Ø· Ù†ÙØ³Ù‡ ÙÙŠ Ø­ÙŠØ§ØªÙ‡ Ø§Ù„Ø¨Ø§Ù„ØºØ©.\n",
        "\n",
        "Ù„ØªØ­Ù„ÙŠÙ„ Ù‡Ø°Ù‡ Ø§Ù„ØªØ£Ø«ÙŠØ±Ø§Øª Ø¨Ø´ÙƒÙ„ Ø¯Ù‚ÙŠÙ‚ØŒ Ø·ÙˆØ±Øª Ø±Ø§Ø¨Ø·Ø© Ø§Ù„Ø¹Ù„Ø§Ø¬ Ø§Ù„Ù…Ø§Ù„ÙŠ\n",
        "(Financial Therapy Association) Ø£Ø¯Ø§Ø© ØªØ³Ù…Ù‰ Ù…Ø®Ø·Ø· Ø§Ù„Ø¬ÙŠÙ†ÙˆÙ… Ø§Ù„Ù…Ø§Ù„ÙŠ (Money Genogram)ØŒ\n",
        "ÙˆÙ‡Ùˆ Ù†Ù…ÙˆØ°Ø¬ ÙŠÙØ³ØªØ®Ø¯Ù… Ù„ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…Ø§Ù„ÙŠØ© Ø¯Ø§Ø®Ù„ Ø§Ù„Ø¹Ø§Ø¦Ù„Ø©.\n",
        "\n",
        "ØªØªØ¶Ù…Ù† Ù‡Ø°Ù‡ Ø§Ù„Ø£Ø¯Ø§Ø©:\n",
        "\n",
        "Ø±Ø³Ù… Ø´Ø¬Ø±Ø© Ø¹Ø§Ø¦Ù„ÙŠØ©.\n",
        "ØªØµÙ†ÙŠÙ Ø£ÙØ±Ø§Ø¯ Ø§Ù„Ø¹Ø§Ø¦Ù„Ø© ÙˆÙÙ‚Ø§ Ù„Ù„Ø£Ø¨Ø¹Ø§Ø¯ Ø§Ù„Ø«Ù„Ø§Ø«Ø© Ù„Ù„Ø¹Ù„Ø§Ù‚Ø© Ø¨Ø§Ù„Ù…Ø§Ù„ (A ØŒU ØŒM).\n",
        "ØªØ­Ø¯ÙŠØ¯ Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³Ù„ÙˆÙƒ Ø§Ù„Ù…Ø§Ù„ÙŠ Ù„ÙƒÙ„ ÙØ±Ø¯ ØµØ­ÙŠØ§ (+) Ø£Ùˆ ØºÙŠØ± ØµØ­ÙŠ (-).\n",
        "Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ Ø¥Ø°Ø§ Ù†Ø´Ø£ Ø´Ø®Øµ ÙÙŠ Ø¹Ø§Ø¦Ù„Ø©\n",
        "Ø§Ø¹ØªØ§Ø¯Øª Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ù†ÙØ§Ù‚ Ø§Ù„Ù…ÙØ±Ø·ØŒ ÙÙ‚Ø¯ ÙŠÙƒÙˆÙ† Ù„Ø¯ÙŠÙ‡ Ù…ÙŠÙ„ Ù‚ÙˆÙŠ Ø¥Ù„Ù‰ Ø§ØªØ¨Ø§Ø¹ Ø§Ù„Ù†Ù…Ø· Ù†ÙØ³Ù‡ØŒ\n",
        " Ø£Ùˆ Ø§Ù„Ø¹ÙƒØ³ ØªÙ…Ø§Ù…Ø§ØŒ Ø­ÙŠØ« ÙŠØµØ¨Ø­ Ù…Ù‚ØªØµØ¯Ø§ Ø¨Ø´ÙƒÙ„ Ù…Ø¨Ø§Ù„Øº ÙÙŠÙ‡ ÙƒØ±Ø¯ ÙØ¹Ù„ Ù†ÙØ³ÙŠ.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ShkKHFV-01gy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # # Another Sample\n",
        "\n",
        "# story = \"\"\"\n",
        "# Ù‚Ø±Ø± Ø§Ù„Ù…Ø¬Ù„Ø³ Ø§Ù„Ù‚ÙˆÙ…ÙŠ Ù„Ù„Ø£Ø¬ÙˆØ± ÙÙŠ Ù…ØµØ±ØŒ Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰ Ù„Ø£Ø¬Ø± Ø§Ù„Ø¹Ø§Ù…Ù„ÙŠÙ† Ø¨Ø§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„Ø®Ø§Øµ Ø¥Ù„Ù‰ 7 Ø¢Ù„Ø§Ù Ø¬Ù†ÙŠÙ‡ Ø´Ù‡Ø±ÙŠÙ‹Ø§ Ù…Ù‚Ø§Ø¨Ù„ 6 Ø¢Ù„Ø§Ù Ø¬Ù†ÙŠÙ‡ØŒ Ø¹Ù„Ù‰ Ø£Ù† ÙŠØªÙ… ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø²ÙŠØ§Ø¯Ø© Ø§Ø¹ØªØ¨Ø§Ø±Ù‹Ø§ Ù…Ù† 1 Ù…Ø§Ø±Ø³ 2025.\n",
        "# ÙƒÙ…Ø§ Ù‚Ø±Ø± Ø§Ù„Ù…Ø¬Ù„Ø³ Ø£Ù† ÙŠÙƒÙˆÙ† Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰ Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ø¹Ù„Ø§ÙˆØ© Ø§Ù„Ø¯ÙˆØ±ÙŠØ© Ù„Ù„Ø¹Ø§Ù…Ù„ÙŠÙ† Ø¨Ø§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„Ø®Ø§Øµ 250 Ø¬Ù†ÙŠÙ‡Ù‹Ø§ Ø´Ù‡Ø±ÙŠÙ‹Ø§ØŒ ÙˆÙ„Ø£ÙˆÙ„ Ù…Ø±Ø© ÙŠÙ‚Ø±Ø± Ø§Ù„Ù…Ø¬Ù„Ø³ Ø§Ù„Ù‚ÙˆÙ…ÙŠ Ù„Ù„Ø£Ø¬ÙˆØ± ÙˆØ¶Ø¹ Ø­Ø¯ Ø£Ø¯Ù†Ù‰ Ù„Ù„Ø£Ø¬Ø± Ù„Ù„Ø¹Ù…Ù„ Ø§Ù„Ù…Ø¤Ù‚Øª \"Ø¬Ø²Ø¡ Ù…Ù† Ø§Ù„ÙˆÙ‚Øª\"ØŒ Ø¨Ø­ÙŠØ« Ù„Ø§ ÙŠÙ‚Ù„ Ø£Ø¬Ø±Ù‡Ù… Ø¹Ù† 28 Ø¬Ù†ÙŠÙ‡Ù‹Ø§ ØµØ§ÙÙŠÙ‹Ø§ ÙÙŠ Ø§Ù„Ø³Ø§Ø¹Ø©ØŒ ÙˆØ°Ù„Ùƒ ÙˆÙÙ‚Ù‹Ø§ Ù„ØªØ¹Ø±ÙŠÙÙ‡Ù… Ø§Ù„ÙˆØ§Ø±Ø¯ ÙÙŠ Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ø¹Ù…Ù„.\n",
        "# ÙˆÙ‚Ø§Ù„Øª ÙˆØ²ÙŠØ±Ø© Ø§Ù„ØªØ®Ø·ÙŠØ· ÙˆØ§Ù„ØªÙ†Ù…ÙŠØ© Ø§Ù„Ø§Ù‚ØªØµØ§Ø¯ÙŠØ© ÙˆØ§Ù„ØªØ¹Ø§ÙˆÙ† Ø§Ù„Ø¯ÙˆÙ„ÙŠØŒ Ø±Ø§Ù†ÙŠØ§ Ø§Ù„Ù…Ø´Ø§Ø·ØŒ Ø¥Ù† Ø±ÙØ¹ Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰ Ù„Ù„Ø£Ø¬ÙˆØ± ÙŠØ£ØªÙŠ ÙÙŠ Ø¥Ø·Ø§Ø± Ø§Ù„Ø­Ø±Øµ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ù„Ù„Ù…Ø³ØªØ¬Ø¯Ø§Øª Ø§Ù„Ø§Ù‚ØªØµØ§Ø¯ÙŠØ© Ø§Ù„Ø±Ø§Ù‡Ù†Ø©ØŒ Ø¨Ù…Ø§ ÙŠØ¹Ø²Ø² Ø§Ù„Ø§Ø³ØªÙ‚Ø±Ø§Ø± Ø§Ù„Ø§Ù‚ØªØµØ§Ø¯ÙŠ ÙˆØ§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠØŒ Ù…Ø¶ÙŠÙØ© Ø£Ù† Ø°Ù„Ùƒ ÙŠØªØ³Ù‚ Ù…Ø¹ Ø§Ù„Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„Ø¯ÙˆÙ„ÙŠØ©ØŒ Ø­ÙŠØ« ØªØ¤ÙƒØ¯ Ù…Ù†Ø¸Ù…Ø© Ø§Ù„Ø¹Ù…Ù„ Ø§Ù„Ø¯ÙˆÙ„ÙŠØ© Ø¹Ù„Ù‰ Ø¶Ø±ÙˆØ±Ø© Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰ Ù„Ù„Ø£Ø¬ÙˆØ± Ø¹Ù„Ù‰ Ø£Ø³Ø§Ø³ Ø¯ÙˆØ±ÙŠØŒ Ù„Ø­Ù…Ø§ÙŠØ© Ø§Ù„Ù‚ÙˆØ© Ø§Ù„Ø´Ø±Ø§Ø¦ÙŠØ© Ù„Ù„Ø£Ø³Ø±ØŒ ÙˆØ§Ø³ØªÙŠØ¹Ø§Ø¨ Ø§Ù„ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø§Ù‚ØªØµØ§Ø¯ÙŠØ© Ø§Ù„ØªØ¯Ø±ÙŠØ¬ÙŠØ©.\n",
        "# \"\"\""
      ],
      "metadata": {
        "id": "A4y-imWZYXhn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Details Extraction"
      ],
      "metadata": {
        "id": "_Hkuh7dM08Zk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "StoryCategory = Literal[\n",
        "    \"politics\", \"sports\", \"art\", \"technology\", \"economy\",\n",
        "    \"health\", \"entertainment\", \"science\",\n",
        "    \"not_specified\"\n",
        "]\n",
        "\n",
        "EntityType = Literal[\n",
        "    \"person-male\", \"person-female\", \"location\", \"organization\", \"event\", \"time\",\n",
        "    \"quantity\", \"money\", \"product\", \"law\", \"disease\", \"artifact\", \"not_specified\"\n",
        "]\n",
        "\n",
        "class Entity(BaseModel):\n",
        "    entity_value: str = Field(..., description=\"The actual name or value of the entity.\")\n",
        "    entity_type: EntityType = Field(..., description=\"The type of recognized entity.\")\n",
        "\n",
        "class NewsDetails(BaseModel):\n",
        "    story_title: str = Field(..., min_length=5, max_length=300,\n",
        "                             description=\"A fully informative and SEO optimized title of the story.\")\n",
        "\n",
        "    story_keywords: List[str] = Field(..., min_items=1,\n",
        "                                      description=\"Relevant keywords associated with the story.\")\n",
        "\n",
        "    story_summary: List[str] = Field(\n",
        "                                    ..., min_items=1, max_items=5,\n",
        "                                    description=\"Summarized key points about the story (1-5 points).\"\n",
        "                                )\n",
        "\n",
        "    story_category: StoryCategory = Field(..., description=\"Category of the news story.\")\n",
        "\n",
        "    story_entities: List[Entity] = Field(..., min_items=1, max_items=10,\n",
        "                                        description=\"List of identified entities in the story.\")\n"
      ],
      "metadata": {
        "id": "5Uq6gtPO0-np"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "details_extraction_messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"\\n\".join([\n",
        "            \"You are an NLP data paraser.\",\n",
        "            \"You will be provided by an Arabic text associated with a Pydantic scheme.\",\n",
        "            \"Generate the ouptut in the same story language.\",\n",
        "            \"You have to extract JSON details from text according the Pydantic details.\",\n",
        "            \"Extract details as mentioned in text.\",\n",
        "            \"Do not generate any introduction or conclusion.\"\n",
        "        ])\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"\\n\".join([\n",
        "            \"## Story:\",\n",
        "            story.strip(),\n",
        "            \"\",\n",
        "\n",
        "            \"## Pydantic Details:\",\n",
        "            json.dumps(\n",
        "                NewsDetails.model_json_schema(), ensure_ascii=False\n",
        "            ),\n",
        "            \"\",\n",
        "\n",
        "            \"## Story Details:\",\n",
        "            \"```json\"\n",
        "        ])\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "N6838nmZ5Nsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Translation"
      ],
      "metadata": {
        "id": "w1E3YIOW-f_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TranslatedStory(BaseModel):\n",
        "    translated_title: str = Field(..., min_length=5, max_length=300,\n",
        "                                  description=\"Suggested translated title of the news story.\")\n",
        "    translated_content: str = Field(..., min_length=5,\n",
        "                                    description=\"Translated content of the news story.\")\n",
        "\n",
        "targeted_lang = \"English\"\n",
        "\n",
        "translation_messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"\\n\".join([\n",
        "            \"You are a professional translator.\",\n",
        "            \"You will be provided by an Arabic text.\",\n",
        "            \"You have to translate the text into the `Targeted Language`.\",\n",
        "            \"Follow the provided Scheme to generate a JSON\",\n",
        "            \"Do not generate any introduction or conclusion.\"\n",
        "        ])\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\":  \"\\n\".join([\n",
        "            \"## Story:\",\n",
        "            story.strip(),\n",
        "            \"\",\n",
        "\n",
        "            \"## Pydantic Details:\",\n",
        "            json.dumps( TranslatedStory.model_json_schema(), ensure_ascii=False ),\n",
        "            \"\",\n",
        "\n",
        "            \"## Targeted Language:\",\n",
        "            targeted_lang,\n",
        "            \"\",\n",
        "\n",
        "            \"## Translated Story:\",\n",
        "            \"```json\"\n",
        "\n",
        "        ])\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "y6J9rGpY-jmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "wkVOXy3y7YI_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_id,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype = torch_dtype\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_id)"
      ],
      "metadata": {
        "id": "6Au-Yng57ZZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "9T3NyBjaRn1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = tokenizer.apply_chat_template(\n",
        "    details_extraction_messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "\n",
        "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
        "\n",
        "generated_ids = model.generate(\n",
        "    model_inputs.input_ids,\n",
        "    max_new_tokens=1024,\n",
        "    do_sample=False, top_k=None, temperature=None, top_p=None,\n",
        ")\n",
        "\n",
        "generated_ids = [\n",
        "    output_ids[len(input_ids):]\n",
        "    for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "]\n",
        "\n",
        "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coJEFfI98amn",
        "outputId": "28ee6494-5720-4012-86b0-d7405e6c0d80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "id": "vZfYD0K5-SLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = tokenizer.apply_chat_template(\n",
        "    translation_messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "\n",
        "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
        "\n",
        "generated_ids = model.generate(\n",
        "    model_inputs.input_ids,\n",
        "    max_new_tokens=1024,\n",
        "    do_sample=False, top_k=None, temperature=None, top_p=None,\n",
        ")\n",
        "\n",
        "generated_ids = [\n",
        "    output_ids[len(input_ids):]\n",
        "    for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "]\n",
        "\n",
        "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
      ],
      "metadata": {
        "id": "UqSsu4dg_qYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate OpenAI"
      ],
      "metadata": {
        "id": "nAy8s4WtAJkn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "openai_client = OpenAI(\n",
        "    api_key=userdata.get('openai-colab'),\n",
        "    # base_url=\"http://localhost:8000\"\n",
        ")\n",
        "\n",
        "openai_model_id = \"gpt-4o-mini\""
      ],
      "metadata": {
        "id": "UUzk0STHALVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_completion = openai_client.chat.completions.create(\n",
        "    messages=details_extraction_messages,\n",
        "    model=openai_model_id,\n",
        "    temperature=0.2,\n",
        ")\n",
        "\n",
        "print(chat_completion.choices[0].message.content)"
      ],
      "metadata": {
        "id": "8ADnZXKdAIgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_completion = openai_client.chat.completions.create(\n",
        "    messages=translation_messages,\n",
        "    model=openai_model_id,\n",
        "    temperature=0.2,\n",
        ")\n",
        "\n",
        "print(chat_completion.choices[0].message.content)"
      ],
      "metadata": {
        "id": "W9mH1TX2A9n7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "json.loads(chat_completion.choices[0].message.content)"
      ],
      "metadata": {
        "id": "3seAQTZJEv3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Knowledge Distillation"
      ],
      "metadata": {
        "id": "iWCWwnYiBUY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_data_path = join(data_dir, \"datasets\", \"news-sample.jsonl\")\n",
        "\n",
        "raw_data = []\n",
        "for line in open(raw_data_path):\n",
        "    if line.strip() == \"\":\n",
        "        continue\n",
        "\n",
        "    raw_data.append(\n",
        "        json.loads(line.strip())\n",
        "    )\n",
        "\n",
        "random.Random(101).shuffle(raw_data)\n",
        "\n",
        "print(f\"Raw data: {len(raw_data)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RBNZW5jBvVe",
        "outputId": "e568b953-18cc-4041-b9be-e2f769b877f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw data: 2400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_data[0]['content']"
      ],
      "metadata": {
        "id": "lGPQ_MuwD0aS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cloud_model_id = \"gpt-4o-mini\"\n",
        "price_per_1m_input_tokens = 0.150\n",
        "price_per_1m_output_tokens = 0.600\n",
        "\n",
        "prompt_tokens = 0\n",
        "completion_tokens = 0\n",
        "\n",
        "save_to = join(data_dir, \"datasets\", \"sft.jsonl\")\n",
        "\n",
        "ix = 0\n",
        "for story in tqdm(raw_data):\n",
        "\n",
        "    sample_details_extraction_messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"\\n\".join([\n",
        "                \"You are an NLP data paraser.\",\n",
        "                \"You will be provided by an Arabic text associated with a Pydantic scheme.\",\n",
        "                \"Generate the ouptut in the same story language.\",\n",
        "                \"You have to extract JSON details from text according the Pydantic details.\",\n",
        "                \"Extract details as mentioned in text.\",\n",
        "                \"Do not generate any introduction or conclusion.\"\n",
        "            ])\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"\\n\".join([\n",
        "                \"## Story:\",\n",
        "                story['content'].strip(),\n",
        "                \"\",\n",
        "\n",
        "                \"## Pydantic Details:\",\n",
        "                json.dumps(\n",
        "                    NewsDetails.model_json_schema(), ensure_ascii=False\n",
        "                ),\n",
        "                \"\",\n",
        "\n",
        "                \"## Story Details:\",\n",
        "                \"```json\"\n",
        "            ])\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    response = openai_client.chat.completions.create(\n",
        "                            messages=sample_details_extraction_messages,\n",
        "                            model=cloud_model_id,\n",
        "                            temperature=0.2,\n",
        "                        )\n",
        "\n",
        "    if response.choices[0].finish_reason != \"stop\":\n",
        "        prompt_tokens += response.usage.prompt_tokens\n",
        "        continue\n",
        "\n",
        "    llm_response = response.choices[0].message.content\n",
        "    llm_resp_dict = parse_json(llm_response)\n",
        "\n",
        "    if not llm_resp_dict:\n",
        "        continue\n",
        "\n",
        "    with open(save_to, \"a\", encoding=\"utf8\") as dest:\n",
        "        dest.write(json.dumps({\n",
        "            \"id\": ix,\n",
        "            \"story\": story['content'].strip(),\n",
        "            \"task\": \"Extrat the story details into a JSON.\",\n",
        "            \"output_scheme\": json.dumps( NewsDetails.model_json_schema(), ensure_ascii=False ),\n",
        "            \"response\": llm_resp_dict,\n",
        "        }, ensure_ascii=False, default=str)  + \"\\n\" )\n",
        "\n",
        "    ix += 1\n",
        "    prompt_tokens += response.usage.prompt_tokens\n",
        "    completion_tokens += response.usage.completion_tokens\n",
        "\n",
        "    if(ix % 3) == 0:\n",
        "        cost_input = (prompt_tokens / 1_000_000) * price_per_1m_input_tokens\n",
        "        cost_output = (completion_tokens / 1_000_000) * price_per_1m_output_tokens\n",
        "        total_cost = cost_input + cost_output\n",
        "\n",
        "        print(f\"Iteration {ix}: Total Cost = ${total_cost:.4f} \")"
      ],
      "metadata": {
        "id": "RIQYhOO_DHdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cloud_model_id = \"gpt-4o-mini\"\n",
        "price_per_1m_input_tokens = 0.150\n",
        "price_per_1m_output_tokens = 0.600\n",
        "\n",
        "prompt_tokens = 0\n",
        "completion_tokens = 0\n",
        "\n",
        "save_to = join(data_dir, \"datasets\", \"sft.jsonl\")\n",
        "\n",
        "ix = 0\n",
        "for story in tqdm(raw_data):\n",
        "\n",
        "    for targeted_lang in [\"English\", \"French\"]:\n",
        "        sample_translation_messages = [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"\\n\".join([\n",
        "                    \"You are a professional translator.\",\n",
        "                    \"You will be provided by an Arabic text.\",\n",
        "                    \"You have to translate the text into the `Targeted Language`.\",\n",
        "                    \"Follow the provided Scheme to generate a JSON\",\n",
        "                    \"Do not generate any introduction or conclusion.\"\n",
        "                ])\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"\\n\".join([\n",
        "                    \"## Pydantic Details:\",\n",
        "                    json.dumps( TranslatedStory.model_json_schema(), ensure_ascii=False ),\n",
        "                    \"\",\n",
        "\n",
        "                    \"## Targeted Language or Dialect:\",\n",
        "                    targeted_lang,\n",
        "                    \"\",\n",
        "\n",
        "                    \"## Story:\",\n",
        "                    story['content'].strip(),\n",
        "                    \"\",\n",
        "\n",
        "                    \"## Translated Story:\",\n",
        "                    \"```json\"\n",
        "                ])\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        response = openai_client.chat.completions.create(\n",
        "                                messages=sample_translation_messages,\n",
        "                                model=cloud_model_id,\n",
        "                                temperature=0.2,\n",
        "                            )\n",
        "\n",
        "        if response.choices[0].finish_reason != \"stop\":\n",
        "            prompt_tokens += response.usage.prompt_tokens\n",
        "            continue\n",
        "\n",
        "        llm_response = response.choices[0].message.content\n",
        "        llm_resp_dict = parse_json(llm_response)\n",
        "\n",
        "        if not llm_resp_dict:\n",
        "            continue\n",
        "\n",
        "        with open(save_to, \"a\", encoding=\"utf8\") as dest:\n",
        "            dest.write(json.dumps({\n",
        "                \"id\": ix,\n",
        "                \"story\": story['content'].strip(),\n",
        "\n",
        "                \"output_scheme\": json.dumps( TranslatedStory.model_json_schema(), ensure_ascii=False ),\n",
        "                \"task\": f\"You have to translate the story content into {targeted_lang} associated with a title into a JSON.\",\n",
        "\n",
        "                \"response\": llm_resp_dict,\n",
        "            }, ensure_ascii=False, default=str)  + \"\\n\" )\n",
        "\n",
        "        ix += 1\n",
        "        prompt_tokens += response.usage.prompt_tokens\n",
        "        completion_tokens += response.usage.completion_tokens\n",
        "\n",
        "        if(ix % 3) == 0:\n",
        "            cost_input = (prompt_tokens / 1_000_000) * price_per_1m_input_tokens\n",
        "            cost_output = (completion_tokens / 1_000_000) * price_per_1m_output_tokens\n",
        "            total_cost = cost_input + cost_output\n",
        "\n",
        "            print(f\"Iteration {ix}: Total Cost = ${total_cost:.4f} \")"
      ],
      "metadata": {
        "id": "UEV0rL5LHaIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Format Finetuning Datasets"
      ],
      "metadata": {
        "id": "xBbGFkVEH1X-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sft_data_path = join(data_dir, \"datasets\", \"sft.jsonl\")\n",
        "llm_finetunning_data = []\n",
        "\n",
        "system_message = \"\\n\".join([\n",
        "    \"You are a professional NLP data parser.\",\n",
        "    \"Follow the provided `Task` by the user and the `Output Scheme` to generate the `Output JSON`.\",\n",
        "    \"Do not generate any introduction or conclusion.\"\n",
        "])\n",
        "\n",
        "for line in open(sft_data_path):\n",
        "    if line.strip() == \"\":\n",
        "        continue\n",
        "\n",
        "    rec = json.loads(line.strip())\n",
        "\n",
        "    llm_finetunning_data.append({\n",
        "        \"system\": system_message,\n",
        "        \"instruction\": \"\\n\".join([\n",
        "            \"# Story:\",\n",
        "            rec[\"story\"],\n",
        "\n",
        "            \"# Task:\",\n",
        "            rec[\"task\"],\n",
        "\n",
        "            \"# Output Scheme:\",\n",
        "            rec[\"output_scheme\"],\n",
        "            \"\",\n",
        "\n",
        "            \"# Output JSON:\",\n",
        "            \"```json\"\n",
        "\n",
        "        ]),\n",
        "        \"input\": \"\",\n",
        "        \"output\": \"\\n\".join([\n",
        "            \"```json\",\n",
        "            json.dumps(rec[\"response\"], ensure_ascii=False, default=str),\n",
        "            \"```\"\n",
        "        ]),\n",
        "        \"history\": []\n",
        "    })\n",
        "\n",
        "random.Random(101).shuffle(llm_finetunning_data)"
      ],
      "metadata": {
        "id": "bn4uUuDOIO3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(llm_finetunning_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUXpYjfAIfKb",
        "outputId": "41719811-27dc-4074-adfe-e094a19b3a10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2766"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_sample_sz = 2700\n",
        "\n",
        "train_ds = llm_finetunning_data[:train_sample_sz]\n",
        "eval_ds = llm_finetunning_data[train_sample_sz:]\n",
        "\n",
        "os.makedirs(join(data_dir, \"datasets\", \"llamafactory-finetune-data\"), exist_ok=True)\n",
        "\n",
        "with open(join(data_dir, \"datasets\", \"llamafactory-finetune-data\", \"train.json\"), \"w\") as dest:\n",
        "    json.dump(train_ds, dest, ensure_ascii=False, default=str)\n",
        "\n",
        "with open(join(data_dir, \"datasets\", \"llamafactory-finetune-data\", \"val.json\"), \"w\", encoding=\"utf8\") as dest:\n",
        "    json.dump(eval_ds, dest, ensure_ascii=False, default=str)"
      ],
      "metadata": {
        "id": "DvWmfB-QPJUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "join(data_dir, \"datasets\", \"llamafactory-finetune-data\", \"val.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "3mkuT7i6P6Uz",
        "outputId": "642d18b9-5acf-43a1-8edb-1967a59318fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/gdrive/MyDrive/youtube-resources/llm-finetuning/datasets/llamafactory-finetune-data/val.json'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finetuning"
      ],
      "metadata": {
        "id": "XoBc56BiRDLO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Configure LLaMA-Factory for the new datasets\n",
        "\n",
        "# # update /content/LLaMA-Factory/data/dataset_info.json and append\n",
        "# ```\n",
        "   \"news_finetune_train\": {\n",
        "        \"file_name\": \"/gdrive/MyDrive/youtube-resources/llm-finetuning/datasets/llamafactory-finetune-data/train.json\",\n",
        "        \"columns\": {\n",
        "            \"prompt\": \"instruction\",\n",
        "            \"query\": \"input\",\n",
        "            \"response\": \"output\",\n",
        "            \"system\": \"system\",\n",
        "            \"history\": \"history\"\n",
        "        }\n",
        "    },\n",
        "    \"news_finetune_val\": {\n",
        "        \"file_name\": \"/gdrive/MyDrive/youtube-resources/llm-finetuning/datasets/llamafactory-finetune-data/val.json\",\n",
        "        \"columns\": {\n",
        "            \"prompt\": \"instruction\",\n",
        "            \"query\": \"input\",\n",
        "            \"response\": \"output\",\n",
        "            \"system\": \"system\",\n",
        "            \"history\": \"history\"\n",
        "        }\n",
        "    }\n",
        "# ```\n",
        "\n",
        "# https://wandb.ai/mr-bakrianoo/llamafactory/runs/apwbkni9\n",
        "# https://wandb.ai/mr-bakrianoo/llamafactory/runs/c5tf0q90"
      ],
      "metadata": {
        "id": "PMTk3i2fQKk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/LLaMA-Factory/examples/train_lora/news_finetune.yaml\n",
        "\n",
        "### model\n",
        "model_name_or_path: Qwen/Qwen2.5-1.5B-Instruct\n",
        "trust_remote_code: true\n",
        "\n",
        "### method\n",
        "stage: sft\n",
        "do_train: true\n",
        "finetuning_type: lora\n",
        "lora_rank: 64\n",
        "lora_target: all\n",
        "\n",
        "### dataset\n",
        "dataset: news_finetune_train\n",
        "eval_dataset: news_finetune_val\n",
        "template: qwen\n",
        "cutoff_len: 3500\n",
        "# max_samples: 50\n",
        "overwrite_cache: true\n",
        "preprocessing_num_workers: 16\n",
        "\n",
        "### output\n",
        "# resume_from_checkpoint: /gdrive/MyDrive/youtube-resources/llm-finetuning/models/checkpoint-1500\n",
        "output_dir: /gdrive/MyDrive/youtube-resources/llm-finetuning/models/\n",
        "logging_steps: 10\n",
        "save_steps: 500\n",
        "plot_loss: true\n",
        "# overwrite_output_dir: true\n",
        "\n",
        "### train\n",
        "per_device_train_batch_size: 1\n",
        "gradient_accumulation_steps: 4\n",
        "learning_rate: 1.0e-4\n",
        "num_train_epochs: 3.0\n",
        "lr_scheduler_type: cosine\n",
        "warmup_ratio: 0.1\n",
        "bf16: true\n",
        "ddp_timeout: 180000000\n",
        "\n",
        "### eval\n",
        "# val_size: 0.1\n",
        "per_device_eval_batch_size: 1\n",
        "eval_strategy: steps\n",
        "eval_steps: 100\n",
        "\n",
        "report_to: wandb\n",
        "run_name: newsx-finetune-llamafactory\n",
        "\n",
        "push_to_hub: true\n",
        "export_hub_model_id: \"bakrianoo/news-analyzer\"\n",
        "hub_private_repo: true\n",
        "hub_strategy: checkpoint\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCWgFdsoQrow",
        "outputId": "4a48543d-52b6-4a9f-ef72-84ffb674a56a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/LLaMA-Factory/examples/train_lora/news_finetune.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd LLaMA-Factory/ && llamafactory-cli train /content/LLaMA-Factory/examples/train_lora/news_finetune.yaml"
      ],
      "metadata": {
        "id": "Wkigp2KPVgqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## New Finetuned Model Evaluation"
      ],
      "metadata": {
        "id": "GeKBEof7Xa6a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_id,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype = torch_dtype\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_id)"
      ],
      "metadata": {
        "id": "WXG625DZXcrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finetuned_model_id = \"/gdrive/MyDrive/youtube-resources/llm-finetuning/models\"\n",
        "model.load_adapter(finetuned_model_id)"
      ],
      "metadata": {
        "id": "dXzybVp2X048"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_resp(messages):\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
        "\n",
        "    generated_ids = model.generate(\n",
        "        model_inputs.input_ids,\n",
        "        max_new_tokens=1024,\n",
        "        do_sample=False, top_k=None, temperature=None, top_p=None,\n",
        "    )\n",
        "\n",
        "    generated_ids = [\n",
        "        output_ids[len(input_ids):]\n",
        "        for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "    ]\n",
        "\n",
        "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "    return response\n",
        "\n",
        "response = generate_resp(translation_messages)"
      ],
      "metadata": {
        "id": "M45kdkH-XXmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parse_json(response)"
      ],
      "metadata": {
        "id": "9IghFtP0XtvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tip for Qwen2.5\n",
        "\n",
        "Qwen2.5 oftenly produce chinese characters with some responses. To skip this, use the next class to generate responses.\n",
        "\n",
        "Source:\n",
        "`https://jupyter267.medium.com/how-to-eliminate-the-chance-of-generating-chinese-in-qwen-2-5-2cf919bb0fdc`\n",
        "\n"
      ],
      "metadata": {
        "id": "R4gFkGQA4_BW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator:\n",
        "    def __init__(self, model, tokenizer):\n",
        "\n",
        "        self.model, self.tokenizer = model, tokenizer\n",
        "        self.mask = None\n",
        "\n",
        "    def generate(self, messages:list, max_new_tokens: int=2000, temperature:float=0.1):\n",
        "\n",
        "        def logits_processor(token_ids, logits):\n",
        "          # logits_processor default recieve the logits which is the score matrix of each time-step\n",
        "          \"\"\"\n",
        "              A processor to ban Chinese character\n",
        "          \"\"\"\n",
        "          if self.mask is None:\n",
        "              # as we don't know where the Chinses tokens locate at which index\n",
        "              # in the vocabulary but we know how it looks like and the range of it\n",
        "\n",
        "              # decode all the tokens in the vocabulary in order\n",
        "              token_ids = torch.arange(logits.size(-1))\n",
        "              decoded_tokens = self.tokenizer.batch_decode(token_ids.unsqueeze(1), skip_special_tokens=True)\n",
        "\n",
        "              # create a mask tensor to exclude positions of Chinese characters.\n",
        "              # since this process uses a for loop and is time-consuming,\n",
        "              # the result will be stored as a property for later use to ensure it only runs once.\n",
        "              self.mask = torch.tensor([\n",
        "                  # loop through each token in the vocabulary and compare it to Chinese characters.\n",
        "                  any(0x4E00 <= ord(c) <= 0x9FFF or 0x3400 <= ord(c) <= 0x4DBF or 0xF900 <= ord(c) <= 0xFAFF for c in\n",
        "                      token)\n",
        "                  for token in decoded_tokens\n",
        "              ])\n",
        "\n",
        "          # mask the score by - inf\n",
        "          logits[:, self.mask] = -float(\"inf\")\n",
        "          return logits\n",
        "\n",
        "        # this step transforms the messages into a string,\n",
        "        # adding special tokens e.g separate tokens between system content user queries\n",
        "        text = self.tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True,\n",
        "        )\n",
        "\n",
        "        model_inputs = self.tokenizer([text], return_tensors=\"pt\").to(self.model.device)\n",
        "\n",
        "        generated_ids = self.model.generate(\n",
        "            **model_inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=temperature,\n",
        "            # add the logits_processor here\n",
        "            logits_processor=[logits_processor]\n",
        "        )\n",
        "        generated_ids = [\n",
        "            output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "        ]\n",
        "        response = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "        return response"
      ],
      "metadata": {
        "id": "7LtXq5y95CA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define an object\n",
        "llm = Generator(model, tokenizer)\n",
        "\n",
        "# generate a response without chinese characters\n",
        "response = llm.generate(details_extraction_messages)\n",
        "print( parse_json(response) )\n",
        "\n",
        "response = llm.generate(translation_messages)\n",
        "print( parse_json(response) )"
      ],
      "metadata": {
        "id": "nsA7F8wD5XVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cost Estimation"
      ],
      "metadata": {
        "id": "oC9_nqQaZKWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "from faker import Faker\n",
        "import random\n",
        "from datetime import datetime\n",
        "\n",
        "start_time = datetime.now()\n",
        "fake = Faker('ar')\n",
        "\n",
        "input_tokens = 0\n",
        "output_tokens = 0\n",
        "\n",
        "for i in tqdm(range(30)):\n",
        "    prompt = fake.text(max_nb_chars=random.randint(150, 200))\n",
        "\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": prompt,\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    response = generate_resp(messages)\n",
        "\n",
        "    input_tokens += len(tokenizer.apply_chat_template(messages))\n",
        "    output_tokens += len(tokenizer.encode(response))\n",
        "\n",
        "total_time = (datetime.now() - start_time).total_seconds()\n",
        "\n",
        "print(f\"Total Time: {total_time} seconds\")\n",
        "print(f\"Input Tokens: {input_tokens}\")\n",
        "print(f\"Output Tokens: {output_tokens}\")\n",
        "print(f\"Total Tokens: {input_tokens + output_tokens}\")"
      ],
      "metadata": {
        "id": "FK_FWdKeZMnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Total Time: 387.838115 seconds\n",
        "# Input Tokens: 2446\n",
        "# Output Tokens: 7375\n",
        "# Total Tokens: 9821"
      ],
      "metadata": {
        "id": "l5qgKwWWakuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "9821  / 388"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQRCACPaZwOf",
        "outputId": "75ece51c-9575-4d74-8687-87f8ba16a904"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25.311855670103093"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## vLLM"
      ],
      "metadata": {
        "id": "GOC2oP97bkOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_model_id = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "adapter_model_id = \"/gdrive/MyDrive/youtube-resources/llm-finetuning/models\"\n",
        "\n",
        "\n",
        "!nohup vllm serve \"{base_model_id}\" --dtype=half --gpu-memory-utilization 0.8 --max_lora_rank 64 --enable-lora --lora-modules news-lora=\"{adapter_model_id}\" &\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zr72ssnKbrwH",
        "outputId": "0801150e-5793-42d1-e91c-bcac13ce25ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tail -n 30 nohup.out"
      ],
      "metadata": {
        "id": "pHMHHfA9dLta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference"
      ],
      "metadata": {
        "id": "TBZ2Du8tdjSy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
        "\n",
        "prompt = tokenizer.apply_chat_template(\n",
        "    translation_messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True\n",
        ")"
      ],
      "metadata": {
        "id": "n6Gor_fVdW-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vllm_model_id = \"news-lora\"\n",
        "\n",
        "llm_response = requests.post(\"http://localhost:8000/v1/completions\", json={\n",
        "    \"model\": vllm_model_id,\n",
        "    \"prompt\": prompt,\n",
        "    \"max_tokens\": 1000,\n",
        "    \"temperature\": 0.3\n",
        "})\n",
        "\n",
        "llm_response.json()"
      ],
      "metadata": {
        "id": "JYPDkJ_Adp7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Testing"
      ],
      "metadata": {
        "id": "a3p1nGLyedA8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile locust.py\n",
        "\n",
        "import random\n",
        "import json\n",
        "from locust import HttpUser, task, between, constant\n",
        "from transformers import AutoTokenizer\n",
        "from faker import Faker\n",
        "\n",
        "fake = Faker('ar')\n",
        "\n",
        "class CompletionLoadTest(HttpUser):\n",
        "    wait_time = between(1, 3)\n",
        "\n",
        "    @task\n",
        "    def post_completion(self):\n",
        "        model_id = \"news-lora\"\n",
        "        prompt = fake.text(max_nb_chars=random.randint(150, 200))\n",
        "\n",
        "        message = {\n",
        "            \"model\": model_id,\n",
        "            \"prompt\": prompt,\n",
        "            \"max_tokens\": 512,\n",
        "            \"temperature\": 0.3\n",
        "        }\n",
        "\n",
        "        llm_response = self.client.post(\"/v1/completions\", json=message)\n",
        "\n",
        "        if llm_response.status_code == 200:\n",
        "            with open(\"./vllm_tokens.txt\", \"a\") as dest:\n",
        "                dest.write(json.dumps({\n",
        "                    \"prompt\": prompt,\n",
        "                    \"response\": llm_response.json()[\"choices\"][0][\"text\"],\n",
        "                }, ensure_ascii=False) + \"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOZYakDneeoD",
        "outputId": "5bef29a9-235a-4032-8f80-f16ecdd3c1d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting locust.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!locust --headless -f locust.py --host=http://localhost:8000 -u 20 -r 1 -t \"60s\" --html=locust_results.html"
      ],
      "metadata": {
        "id": "DYPw83hzfeNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vllm_tokens = [\n",
        "    json.loads(line.strip())\n",
        "    for line in open(\"./vllm_tokens.txt\") if line.strip() != \"\"\n",
        "]"
      ],
      "metadata": {
        "id": "oZFSPbufiYbk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model_id = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
        "\n",
        "total_input_tokens = sum([ len(tokenizer.encode(rec['prompt'])) for rec in vllm_tokens ])\n",
        "total_output_tokens = sum([ len(tokenizer.encode(rec['response'])) for rec in vllm_tokens ])\n",
        "\n",
        "print(f\"Total Input Tokens: {total_input_tokens}\")\n",
        "print(f\"Total Output Tokens: {total_output_tokens}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCwHMzVnigg9",
        "outputId": "07a15b5d-93b9-46ff-96d8-84241066bc75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Input Tokens: 2662\n",
            "Total Output Tokens: 37840\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "37840 / 60"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOpAbHLSioBz",
        "outputId": "424330d7-ed80-40bd-97e1-ab5a5d16142a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "630.6666666666666"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    }
  ]
}